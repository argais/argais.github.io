<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Linuxaria Gourmet</title>
    <description>...linux, python, cloud computing, cooking, music...
</description>
    <link>//</link>
    <atom:link href="//feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Sun, 08 May 2016 20:45:35 -0300</pubDate>
    <lastBuildDate>Sun, 08 May 2016 20:45:35 -0300</lastBuildDate>
    <generator>Jekyll v3.1.2</generator>
    
      <item>
        <title>Generate SSH Public Key from Private Key</title>
        <description>&lt;p&gt;I’ll be posting small tidbits from time to time, as not so often used stuff like this is easily forgotten, and my google-fu always takes me to different places :) So, using the blog as a notebook of sorts. Could use gists, 
but I dump so much random stuff in there that I better not use those for reference.&lt;/p&gt;

&lt;p&gt;So, this is for when you get that .pem key from aws or somewhere else, and for some weird reason you need the pubkey for it!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ssh-keygen -y -f your_private_key.pem &amp;gt; your_public_key.pub
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That was hard huh? Yeah I know :D&lt;/p&gt;

</description>
        <pubDate>Sun, 08 May 2016 17:42:00 -0300</pubDate>
        <link>//tidbits/2016/05/08/Generate-SSH-Public-key-from-private-key.html</link>
        <guid isPermaLink="true">//tidbits/2016/05/08/Generate-SSH-Public-key-from-private-key.html</guid>
        
        <category>linux</category>
        
        <category>ssh</category>
        
        
        <category>tidbits</category>
        
      </item>
    
      <item>
        <title>Cloudtrail -&gt; S3 -&gt; Lambda -&gt; Elasticsearch Service</title>
        <description>&lt;p&gt;Cloudtrail, we all need it, and hey, its fairly easy to check stuff on the console.&lt;/p&gt;

&lt;p&gt;Now, if you have multiple AWS accounts, it starts to get hairy, nobody wants to keep jumping over account consoles for that right?&lt;/p&gt;

&lt;p&gt;CLI magic is good, but sometimes, we just want something easy to use, or we want to correlate that data with something else. The usual path would be to setup an ELK stack and be done with it, its simple, it works, but its one more thing to manage.&lt;/p&gt;

&lt;p&gt;AWS now has Elasticsearch as a service, so the E part of the stack is covered, Kibana comes with it, there goes our K, so we’re only missing an L now.&lt;/p&gt;

&lt;p&gt;Cloudtrail logs are stored on S3, now we just need some way to get those logs into Elasticsearch.&lt;/p&gt;

&lt;h2 id=&quot;enter-lambda--s3-events&quot;&gt;Enter Lambda + S3 events!&lt;/h2&gt;

&lt;p&gt;We can setup an S3 a trigger, so whenever a file is put on our bucket we get a call to a lambda function.
That gives us a completely serverless solution for this issue, sweet huh?!&lt;/p&gt;

&lt;p&gt;So our todo list is:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;provision an AWS Elasticsearch Service cluster&lt;/li&gt;
  &lt;li&gt;create a Lambda function that receives S3 events, gets the bucket and sends the json info into Elasticsearch&lt;/li&gt;
  &lt;li&gt;set the S3 event to trigger the lambda function&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;warnings-and-requirements&quot;&gt;Warnings and Requirements&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;I’m assuming at least a basic level of AWS expertise here (how to use the consoles, and click on buttons!), that said, https://docs.aws.amazon.com is an awesome resource ;)&lt;/li&gt;
  &lt;li&gt;I’m also assuming that you already set up cloudtrail and it is saving logs into a s3 bucket just fine.&lt;/li&gt;
  &lt;li&gt;This is a basic how-to, all open security (every time you run all open in production a kitten dies! think of the kittens!), read the docs on lambda, s3, elasticsearch and go with the good old least privilege rule.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;elaaaaaaaaaaaaaaaaaaaaaaaastic-search&quot;&gt;Elaaaaaaaaaaaaaaaaaaaaaaaastic Search&lt;/h2&gt;

&lt;p&gt;There’s plenty of posts on the internet about sharding and replication and stuff on Elasticsearch, so I’ll keep this short.&lt;/p&gt;

&lt;p&gt;If this is important to you, get two servers at least, and replicate between regions.
For this post we will keep this simple, just launch a single server in your cluster, even a t2 instance should be ok, in prod, with a couple accounts, I’m using a pair m3.medium nodes (for 90 really active aws accounts). YMMV.&lt;/p&gt;

&lt;p&gt;Keep note of your endpoint url, for your lambda function, and your kibana url, for.. well.. accessing kibana of course :)&lt;/p&gt;

&lt;p&gt;Now lets add a template for our indexes, so Kibana doesnt cry a river later about analysed fields.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PUT /_template/logstash
{
  &quot;template&quot; : &quot;logstash-*&quot;,
  &quot;settings&quot; : {
    &quot;index.refresh_interval&quot; : &quot;5s&quot;
  },
  &quot;mappings&quot; : {
    &quot;_default_&quot; : {
      &quot;_all&quot; : {&quot;enabled&quot; : true, &quot;omit_norms&quot; : true},
      &quot;dynamic_templates&quot; : [ {
        &quot;message_field&quot; : {
          &quot;match&quot; : &quot;message&quot;,
          &quot;match_mapping_type&quot; : &quot;string&quot;,
          &quot;mapping&quot; : {
            &quot;type&quot; : &quot;string&quot;, &quot;index&quot; : &quot;analyzed&quot;, &quot;omit_norms&quot; : true,
            &quot;fielddata&quot; : { &quot;format&quot; : &quot;enabled&quot; }
          }
        }
      }, {
        &quot;string_fields&quot; : {
          &quot;match&quot; : &quot;*&quot;,
          &quot;match_mapping_type&quot; : &quot;string&quot;,
          &quot;mapping&quot; : {
            &quot;type&quot; : &quot;string&quot;, &quot;index&quot; : &quot;analyzed&quot;, &quot;omit_norms&quot; : true,
            &quot;fielddata&quot; : { &quot;format&quot; : &quot;enabled&quot; },
            &quot;fields&quot; : {
              &quot;raw&quot; : {&quot;type&quot;: &quot;string&quot;, &quot;index&quot; : &quot;not_analyzed&quot;, &quot;doc_values&quot; : true, &quot;ignore_above&quot; : 256}
            }
          }
        }
      }, {
        &quot;float_fields&quot; : {
          &quot;match&quot; : &quot;*&quot;,
          &quot;match_mapping_type&quot; : &quot;float&quot;,
          &quot;mapping&quot; : { &quot;type&quot; : &quot;float&quot;, &quot;doc_values&quot; : true }
        }
      }, {
        &quot;double_fields&quot; : {
          &quot;match&quot; : &quot;*&quot;,
          &quot;match_mapping_type&quot; : &quot;double&quot;,
          &quot;mapping&quot; : { &quot;type&quot; : &quot;double&quot;, &quot;doc_values&quot; : true }
        }
      }, {
        &quot;byte_fields&quot; : {
          &quot;match&quot; : &quot;*&quot;,
          &quot;match_mapping_type&quot; : &quot;byte&quot;,
          &quot;mapping&quot; : { &quot;type&quot; : &quot;byte&quot;, &quot;doc_values&quot; : true }
        }
      }, {
        &quot;short_fields&quot; : {
          &quot;match&quot; : &quot;*&quot;,
          &quot;match_mapping_type&quot; : &quot;short&quot;,
          &quot;mapping&quot; : { &quot;type&quot; : &quot;short&quot;, &quot;doc_values&quot; : true }
        }
      }, {
        &quot;integer_fields&quot; : {
          &quot;match&quot; : &quot;*&quot;,
          &quot;match_mapping_type&quot; : &quot;integer&quot;,
          &quot;mapping&quot; : { &quot;type&quot; : &quot;integer&quot;, &quot;doc_values&quot; : true }
        }
      }, {
        &quot;long_fields&quot; : {
          &quot;match&quot; : &quot;*&quot;,
          &quot;match_mapping_type&quot; : &quot;long&quot;,
          &quot;mapping&quot; : { &quot;type&quot; : &quot;long&quot;, &quot;doc_values&quot; : true }
        }
      }, {
        &quot;date_fields&quot; : {
          &quot;match&quot; : &quot;*&quot;,
          &quot;match_mapping_type&quot; : &quot;date&quot;,
          &quot;mapping&quot; : { &quot;type&quot; : &quot;date&quot;, &quot;doc_values&quot; : true }
        }
      }, {
        &quot;geo_point_fields&quot; : {
          &quot;match&quot; : &quot;*&quot;,
          &quot;match_mapping_type&quot; : &quot;geo_point&quot;,
          &quot;mapping&quot; : { &quot;type&quot; : &quot;geo_point&quot;, &quot;doc_values&quot; : true }
        }
      } ],
      &quot;properties&quot; : {
        &quot;@timestamp&quot;: { &quot;type&quot;: &quot;date&quot;, &quot;doc_values&quot; : true },
        &quot;@version&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;index&quot;: &quot;not_analyzed&quot;, &quot;doc_values&quot; : true },
        &quot;geoip&quot;  : {
          &quot;type&quot; : &quot;object&quot;,
          &quot;dynamic&quot;: true,
          &quot;properties&quot; : {
            &quot;ip&quot;: { &quot;type&quot;: &quot;ip&quot;, &quot;doc_values&quot; : true },
            &quot;location&quot; : { &quot;type&quot; : &quot;geo_point&quot;, &quot;doc_values&quot; : true },
            &quot;latitude&quot; : { &quot;type&quot; : &quot;float&quot;, &quot;doc_values&quot; : true },
            &quot;longitude&quot; : { &quot;type&quot; : &quot;float&quot;, &quot;doc_values&quot; : true }
          }
        }
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;All done on the Elasicsearch side.&lt;/p&gt;

&lt;h2 id=&quot;repeat-with-me-laaaaaambda&quot;&gt;Repeat with me, Laaaaaambda!&lt;/h2&gt;

&lt;p&gt;Clone my repo from github https://github.com/argais/cloudtrail_aws_es , now on the same folder you cloned it, on your terminal, install the requests module on the same folder with &lt;code&gt;pip install requests -t .&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Edit s3_lambda_es.py changing the following variables&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;host = the full hostname for your AWS ES endpoint (just the hostname, remove https:// and any trailing /)&lt;/li&gt;
  &lt;li&gt;region = region were your ES cluster is located&lt;/li&gt;
  &lt;li&gt;access_key and secret_key = a key pair to sign your requests to ES (not using environment ones from lambda, for some reason they didnt work for me)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Zip this entire folder (can keep readme, elastic_search_cloudtrail_template.json and .gitignore out).&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Very important! The files must be on the root of the zip, not inside a folder.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Create a new lambda function, I’ll call mine s3-cloudtrail-elasticsearch, because its just plain easy to remember.&lt;/p&gt;

&lt;p&gt;Skip the blueprints, select Python, and for code upload the zipfile you just created.&lt;/p&gt;

&lt;p&gt;Select the role for your function or create a new one (you need access to cloudwatch logs, and read to the s3 bucket), and finish creating the function. If you need help, refer to the aws docs website I linked before.&lt;/p&gt;

&lt;p&gt;In short, our lambda function receives the event from s3, gets the bucket name and key (the log file name), we download the file from there, read its contents and loop over the log events, inserting each one of them into Elasticsearch&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;If you are pointing to an existing Elasticsearch cluster with other indexes in it, you might want to change the index name on the url.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;For each log event we push the event’s json to the Elasticsearch cluster.&lt;/p&gt;

&lt;h2 id=&quot;s3-event&quot;&gt;S3 Event&lt;/h2&gt;

&lt;p&gt;This is the easy one, go to the properties of your S3 bucket that is holding your cloudtrail logs, click on Events, Add Notification, give a name to it, mine will be s3_lambda_event, creative right?&lt;/p&gt;

&lt;p&gt;In the Events line, select ObjectCreated(All), then on Send To select Lambda function, and select your lambda function created on the previous step.&lt;/p&gt;

&lt;h2 id=&quot;let-me-see-it&quot;&gt;Let me see it!&lt;/h2&gt;

&lt;p&gt;At this point, your s3 logs are being fed to Elasticsearch, and you can open Kibana now to see the good stuff.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;If you don’t see anything on Elasticsearch, check the lambda function logs on cloudwatch. There’s a link on the monitoring tab of your lambda function.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;When you open Kibana for the first time it’ll ask you to configure a index pattern.
If you look at our lambda code, the url variable has a /logstash- in it, thats our url pattern, so you really dont have to change anything since Kibana looks for it as default.
Select @timestamp at the Time-field name dropdown and click on Create.&lt;/p&gt;

&lt;p&gt;Theeeere you go! Now click on the discover window to see your data!!!&lt;/p&gt;

&lt;p&gt;But what do I do with this? It doesnt look much better than a bunch of json files in a pretty screen!?!?!!&lt;/p&gt;

&lt;p&gt;Easy little padawan.&lt;/p&gt;

&lt;p&gt;Lets look at how these logs are structured. Here’s an example log entry&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &quot;eventVersion&quot;: &quot;1.03&quot;,
  &quot;userIdentity&quot;: {
    &quot;type&quot;: &quot;IAMUser&quot;,
    &quot;principalId&quot;: &quot;ALALA5HJQX73TDHSDOFUW&quot;,
    &quot;arn&quot;: &quot;arn:aws:iam::801114545221:user/that.user&quot;,
    &quot;accountId&quot;: &quot;801114545221&quot;,
    &quot;accessKeyId&quot;: &quot;AKIAIBAAP55LADSQELQQ&quot;,
    &quot;userName&quot;: &quot;that.user&quot;
  },
  &quot;@timestamp&quot;: &quot;2016-02-20T23:11:23Z&quot;,
  &quot;eventSource&quot;: &quot;ec2.amazonaws.com&quot;,
  &quot;eventName&quot;: &quot;DescribeInstances&quot;,
  &quot;awsRegion&quot;: &quot;us-east-1&quot;,
  &quot;sourceIPAddress&quot;: &quot;52.71.233.53&quot;,
  &quot;userAgent&quot;: &quot;aws-sdk-java/1.9.22 Linux/3.14.48-33.39.amzn1.x86_64 Java_HotSpot(TM)_64-Bit_Server_VM/24.79-b02/1.7.0_79&quot;,
  &quot;requestParameters&quot;: {
    &quot;instancesSet&quot;: {},
    &quot;filterSet&quot;: {},
  },
  &quot;responseElements&quot;: null,
  &quot;requestID&quot;: &quot;2a74375c-f931-4630-b1b9-077d2df11884&quot;,
  &quot;eventID&quot;: &quot;fd943a8d-03e7-4155-9ec1-bd27ad47a803&quot;,
  &quot;eventType&quot;: &quot;AwsApiCall&quot;,
  &quot;recipientAccountId&quot;: &quot;801114545221&quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;What does this tell us? quite a lot actually, userName from accountId, with the accessKeyId, ran a eventName at eventSource, at @timestamp, from sourceIPAddress.
With this data you could make filters and dashboards showing any actions taken in your accounts.&lt;/p&gt;

&lt;p&gt;Examples:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;all actions taken by user&lt;/li&gt;
  &lt;li&gt;most active users&lt;/li&gt;
  &lt;li&gt;most active access keys&lt;/li&gt;
  &lt;li&gt;failed login attempts&lt;/li&gt;
  &lt;li&gt;changes in security groups&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The queries are done using lucene query string syntax, its quite easy to use, so dont fret, type stuff on the search bar and you will get something back, if nothing, some instructions ;)&lt;/p&gt;

&lt;p&gt;On the visualizations tab you can create new graphs based on queries you saved from the discover tab, or new queries.&lt;/p&gt;

&lt;p&gt;Then you can jump on the dashboard tab, and add these visualizations to your dashboard, resizing them as you wish ;)&lt;/p&gt;

&lt;h2 id=&quot;ok-so-what&quot;&gt;Ok… so what?&lt;/h2&gt;

&lt;p&gt;So how is this usefull? Lets say someone deleted a user this week in one of your 200 accounts, all of them are feeding your Elasticsearch cluster.&lt;/p&gt;

&lt;p&gt;At the discover tab you can click on the right top side to select a Time Filter of This Week, and then on the search field you could put +iam +DeleteUser, this will look for these two words in all the log entries, showing you all the users that were deleted in that period.&lt;/p&gt;

&lt;p&gt;This is cool huh?&lt;/p&gt;

&lt;p&gt;A lot easier than jumping around accounts and consoles looking for stuff ;)&lt;/p&gt;
</description>
        <pubDate>Sun, 13 Mar 2016 13:11:00 -0300</pubDate>
        <link>//log_processing/2016/03/13/Cloudtrail-S3-Lambda-Elasticsearch.html</link>
        <guid isPermaLink="true">//log_processing/2016/03/13/Cloudtrail-S3-Lambda-Elasticsearch.html</guid>
        
        <category>linux</category>
        
        <category>logstash</category>
        
        <category>elasticsearch</category>
        
        <category>kibana</category>
        
        <category>devops</category>
        
        <category>cloudtrail</category>
        
        <category>aws</category>
        
        <category>python</category>
        
        
        <category>log_processing</category>
        
      </item>
    
      <item>
        <title>MySQL Output in Logstash</title>
        <description>&lt;h1 id=&quot;why-so-much-trouble&quot;&gt;Why so much trouble?&lt;/h1&gt;

&lt;p&gt;According to people on the #logstash IRC channel, and a bunch of searching on google, MySQL output filter isnt something easy to implement in logstash 1.4, something along the lines of “its a pain to put the jdbc/mysql driver in”.&lt;/p&gt;

&lt;p&gt;It was also said that 1.5 would change the way plugins work, making this an easier task.&lt;/p&gt;

&lt;p&gt;Well, I can’t wait that long, so I’ll hack my way around it, in a reallly roundabout way.&lt;/p&gt;

&lt;p&gt;I’ll have logstash send the processed log as json to a python/flask app that will insert that into my database.&lt;/p&gt;

&lt;p&gt;My final goal is to have my squid logs inside mysql to make a decent tool to monitor internet/bandwidth usage by user/time/ip.&lt;/p&gt;

&lt;p&gt;To make my life easier I’ll start only feeding it the source IP (the computer that made the request), destination domain and cache user (the user or computer that used the proxy).&lt;/p&gt;

&lt;h1 id=&quot;getting-started&quot;&gt;Getting started&lt;/h1&gt;

&lt;p&gt;We will add two new pieces to our ELK stack.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.mysql.com&quot;&gt;MySQL server&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://flask.pocoo.org&quot;&gt;Flask framework web app&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So in the end the entire thing will follow this path:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;|--- squid server ---------------|---- logstash server ------------|
  Squid log -&amp;gt; Logstash forwarder -&amp;gt; Logstash -&amp;gt; Flask App -&amp;gt; MySQL&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;I wont be covering MySQL instalation, refer to their websites or google for it, theres plenty of guides for those.&lt;/p&gt;

&lt;p&gt;Here is the sql to create what we will be using on this guide.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sql&quot; data-lang=&quot;sql&quot;&gt;&lt;span class=&quot;lineno&quot;&gt; 1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;CREATE&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;DATABASE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;squid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt; 2&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;CREATE&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;TABLE&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;squid&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new_table&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;`&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt; 3&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;`&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;INT&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;NOT&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;NULL&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AUTO_INCREMENT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt; 4&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;user&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;`&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;VARCHAR&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;45&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;NULL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt; 5&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;destination_host&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;`&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;VARCHAR&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;60&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;NULL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt; 6&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uri_parameters&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;`&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;VARCHAR&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;NULL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt; 7&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;source_ip&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;`&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;VARCHAR&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;NULL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt; 8&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;response_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;`&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;BIGINT&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;NULL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt; 9&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;date&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;`&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DATETIME&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;NULL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;10&lt;/span&gt;   &lt;span class=&quot;k&quot;&gt;PRIMARY&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;KEY&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h1 id=&quot;lets-get-it-done&quot;&gt;Lets get it done!&lt;/h1&gt;

&lt;p&gt;First things first, lets make an output filter on logstash, to send our logs as json to our app. So, like in the previous posts, create a new file in &lt;code&gt;/etc/logstash/conf.d/&lt;/code&gt; with the following content:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;lineno&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;2&lt;/span&gt;     &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;squid&amp;quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;3&lt;/span&gt; 	    &lt;span class=&quot;n&quot;&gt;http&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;4&lt;/span&gt; 		    &lt;span class=&quot;n&quot;&gt;format&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;json&amp;quot;&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;5&lt;/span&gt; 		    &lt;span class=&quot;n&quot;&gt;http_method&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;post&amp;quot;&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;6&lt;/span&gt; 		    &lt;span class=&quot;n&quot;&gt;url&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;http://localhost:90/&amp;quot;&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;7&lt;/span&gt; 	  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;8&lt;/span&gt;     &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;9&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;This is like saying, “if the type is squid, send all the stuff as json using post to http://localhost:90”. We will be running our Flask App on port 90.&lt;/p&gt;

&lt;p&gt;Lets get flask installed.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;pip install flask&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;That was hard huh? Get the mysql stuff for python installed too, on ubuntu its:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;apt-get install python-mysqldb&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Ok, now that this stuff is installed, lets get our app running, create a new file for our app.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;lineno&quot;&gt; 1&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# import the modules we will be using&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt; 2&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;MySQLdb&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;mysql&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt; 3&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;datetime&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt; 4&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;json&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt; 5&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;flask&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Flask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;request&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;render_template&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;jsonify&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;make_response&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt; 6&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;app&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Flask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__name__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt; 7&lt;/span&gt; 
&lt;span class=&quot;lineno&quot;&gt; 8&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# when we get a POST or PUT on / do this...&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt; 9&lt;/span&gt; &lt;span class=&quot;nd&quot;&gt;@app.route&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;/&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;methods&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;PUT&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;POST&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;10&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_json_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;11&lt;/span&gt; 	&lt;span class=&quot;c&quot;&gt;# put the json data in a variable&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;12&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;log_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;request&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_json&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;force&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;13&lt;/span&gt; 
&lt;span class=&quot;lineno&quot;&gt;14&lt;/span&gt; 	&lt;span class=&quot;c&quot;&gt;# connect to mysql&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;15&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;connection&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mysql&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;connect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;localhost&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;user&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;root&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;passwd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;root&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;db&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;squid&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;16&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;cursor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;connection&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cursor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;17&lt;/span&gt; 
&lt;span class=&quot;lineno&quot;&gt;18&lt;/span&gt; 	&lt;span class=&quot;c&quot;&gt;# get the values into variables, changing them to strings or ints whenever needed&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;19&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;user&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;cache_user&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;20&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;destination_host&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;dst_host&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;21&lt;/span&gt; 	&lt;span class=&quot;c&quot;&gt;# some requests dont have an uri_param field, but we still want them, so, if theres no uri_param key on the json data, make the variable empty&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;22&lt;/span&gt; 	&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;log_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;has_key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;uri_param&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;23&lt;/span&gt;         &lt;span class=&quot;n&quot;&gt;uri_parameters&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;uri_param&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;24&lt;/span&gt;     &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;25&lt;/span&gt;         &lt;span class=&quot;n&quot;&gt;uri_parameters&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;26&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;source_ip&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;src_ip&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;27&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;response_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;response_size&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;28&lt;/span&gt;     &lt;span class=&quot;c&quot;&gt;# format the timestamp data into a format that isnt a royal pain to work with&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;29&lt;/span&gt; 	&lt;span class=&quot;n&quot;&gt;date&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fromtimestamp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;timestamp&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strftime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;%Y-%m-&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%d&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; %H:%M:%S&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;30&lt;/span&gt; 
&lt;span class=&quot;lineno&quot;&gt;31&lt;/span&gt;     &lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;32&lt;/span&gt;         &lt;span class=&quot;n&quot;&gt;sql&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;INSERT INTO squid (user,destination_host,uri_parameters,source_ip,response_size,date) VALUES (&amp;#39;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%s&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;,&amp;#39;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%s&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;,&amp;#39;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%s&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;,&amp;#39;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%s&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;,&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%d&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;,&amp;#39;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%s&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;)&amp;quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;user&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;destination_host&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;uri_parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;source_ip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;response_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;date&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;33&lt;/span&gt; 		&lt;span class=&quot;c&quot;&gt;# insert the data in the database&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;34&lt;/span&gt; 		&lt;span class=&quot;n&quot;&gt;cursor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;execute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sql&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;35&lt;/span&gt;         &lt;span class=&quot;n&quot;&gt;connection&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;commit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;36&lt;/span&gt;     &lt;span class=&quot;k&quot;&gt;except&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;37&lt;/span&gt;     	&lt;span class=&quot;c&quot;&gt;# if something breaks, rollback!&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;38&lt;/span&gt;         &lt;span class=&quot;n&quot;&gt;connection&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rollback&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;39&lt;/span&gt; 
&lt;span class=&quot;lineno&quot;&gt;40&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;connection&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;close&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;41&lt;/span&gt;     &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;42&lt;/span&gt; 
&lt;span class=&quot;lineno&quot;&gt;43&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__name__&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;__main__&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;44&lt;/span&gt; 	&lt;span class=&quot;c&quot;&gt;# run this on all interfaces and on port 90&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;45&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;0.0.0.0&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;90&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;So what does it do? Well, basically, it opens a small webserver on port 90 that expects to receive a json formated input via post (that our logstash server will send).&lt;/p&gt;

&lt;p&gt;Then it connects to the mysql server (yeah, be wise and dont use root as login and password, mine is a temporary isolated virtual machine, so I dont really care), and inserts a new line into the squid table on the squid database (creative aint I?). If it for some reason fails, rollback the changes.&lt;/p&gt;

&lt;p&gt;Start your flask app by running&lt;/p&gt;

&lt;p&gt;&lt;code&gt;python yourfile.py&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Restart your logstash server&lt;/p&gt;

&lt;p&gt;&lt;code&gt;service logstash restart&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TADA!&lt;/strong&gt; Squid logs into mysql!&lt;/p&gt;

&lt;p&gt;##Now you can easily search, group and make reports with it!&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;If you need more data from the logs you can add the fields on the database, and on the field vars and sql var, I am only collecting the bare minimum I need.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Tue, 14 Oct 2014 13:22:14 -0300</pubDate>
        <link>//log_processing/2014/10/14/mysql-output-in-logstash.html</link>
        <guid isPermaLink="true">//log_processing/2014/10/14/mysql-output-in-logstash.html</guid>
        
        <category>linux</category>
        
        <category>logstash</category>
        
        <category>elasticsearch</category>
        
        <category>kibana</category>
        
        <category>devops</category>
        
        <category>mysql</category>
        
        <category>python</category>
        
        <category>flask</category>
        
        
        <category>log_processing</category>
        
      </item>
    
      <item>
        <title>Squid log filtering with ELK stack</title>
        <description>&lt;p&gt;So now that we have our ELK stack working, lets get squid logs on it!&lt;/p&gt;

&lt;p&gt;First of all install the logstash forwarder on your server like the previous post instructed you.&lt;/p&gt;

&lt;p&gt;When you get to editing the forwarder config file ( /etc/logstash-forwarder ) lets change it a bit to:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &quot;network&quot;: {
    &quot;servers&quot;: [ &quot;logstash_server_ip:5000&quot; ],
    &quot;timeout&quot;: 15,
    &quot;ssl ca&quot;: &quot;/etc/pki/tls/certs/logstash-forwarder.crt&quot;
  },
  &quot;files&quot;: [
    {
      &quot;paths&quot;: [
        &quot;/var/log/squid/access.log&quot;
       ],
      &quot;fields&quot;: { &quot;type&quot;: &quot;squid&quot; }
    }
   ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;On your logstash server lets create a new config file&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vi /etc/logstash/conf.d/11-squid.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And put this in it:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;filter {
    if [type] == &quot;squid&quot; {
      grok {
          match =&amp;gt; { &quot;message&quot; =&amp;gt; &quot;%{SYSLOGTIMESTAMP:date} %{HOST:service} %{DATA:squid_instance} %{NUMBER:timestamp}\s+%{NUMBER:request_msec:int} %{IPORHOST:src_ip} %{WORD:cache_result}/%{NUMBER:response_status:int} %{NUMBER:response_size:int} %{WORD:http_method} (%{URIPROTO:http_proto}://)?%{IPORHOST:dst_host}(?::%{POSINT:port:int})?(?:%{URIPATHPARAM:uri_param})? %{DATA:cache_user} %{WORD:request_route}/(%{IPORHOST:forwarded_to}|-) %{GREEDYDATA:content_type}&quot; }
      }
      date {
          match =&amp;gt; [ &quot;timestamp&quot;, &quot;UNIX&quot; ]
      }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Restart both logstash on your server and logstash-forwarder on your squid server.&lt;/p&gt;

&lt;p&gt;Bang error!
My server is complaining that the charset being sent is different then what was expected, so I changed the charset on my input.&lt;/p&gt;

&lt;p&gt;Edit the input file&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vi /etc/logstash/conf.d/01-lumberjack-input.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And change it so it looks like&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;input {
      lumberjack {
        port =&amp;gt; 5000
        type =&amp;gt; &quot;logs&quot;
        ssl_certificate =&amp;gt; &quot;/etc/pki/tls/certs/logstash-forwarder.crt&quot;
        ssl_key =&amp;gt; &quot;/etc/pki/tls/private/logstash-forwarder.key&quot;
    	codec =&amp;gt; plain {
	    	charset =&amp;gt; &quot;ISO-8859-1&quot;
    	}
      }
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now you’re gonna tell me this &lt;strong&gt;still&lt;/strong&gt; isnt working for you.&lt;/p&gt;

&lt;p&gt;Read up on grok, and check if this filter is valid for your squid log.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;One really valueable tool is &lt;a href=&quot;http://grokdebug.herokuapp.com/&quot;&gt;grok debug&lt;/a&gt;, you feed it a line of your log, and start putting the grok filters and you can see the output, thus, validating your grok filter easily.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;If its all working, you should see something like this on the default logstash dashboard on kibana (searching for squid).&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/article_images/2014-10-10-squid-log-filtering-with-elk-stack/squid-kibana-working.png&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;How is this usefull for monitoring squid? Well, different admins have different needs.&lt;/p&gt;

&lt;p&gt;Personally, in our company our users have unrestricted access, kind off, theres filters in squid for adult content, games, stuff that has absolutely nothing to do with work, so we want to know who got their access denied to urls, and who is using the most bandwidth.&lt;/p&gt;

&lt;p&gt;So now you have all that log data in your hands, using simple filters in kibana you can check what a specific user did in a period of time, or check when you have traffic spikes, etc etc…&lt;/p&gt;

&lt;p&gt;Have fun ;)&lt;/p&gt;

</description>
        <pubDate>Fri, 10 Oct 2014 11:05:20 -0300</pubDate>
        <link>//log_processing/2014/10/10/squid-log-filtering-with-elk-stack.html</link>
        <guid isPermaLink="true">//log_processing/2014/10/10/squid-log-filtering-with-elk-stack.html</guid>
        
        <category>linux</category>
        
        <category>logstash</category>
        
        <category>elasticsearch</category>
        
        <category>kibana</category>
        
        <category>devops</category>
        
        
        <category>log_processing</category>
        
      </item>
    
      <item>
        <title>ELK (Elasticsearch, Logstash and Kibana) Stack and Squid</title>
        <description>&lt;h1 id=&quot;intro&quot;&gt;Intro&lt;/h1&gt;

&lt;p&gt;Squid, the mean guy all users hate.
Personally, as an admin, I hate it too, but its a necessary evil.&lt;/p&gt;

&lt;p&gt;Now, checking its logs is hell, whoever devised that timestamp surelly is a mean person :(&lt;/p&gt;

&lt;p&gt;In time I will make a decent squid monitoring solution, its one of my professional goals, but for now I want a better way to look at those logs.&lt;/p&gt;

&lt;p&gt;I also want to have centralized logging and an easy way to look at those, and that brings me to the perfect excuse to go fiddle with the &lt;a href=&quot;www.elasticsearch.org&quot;&gt;ELK stack&lt;/a&gt; (Elasticsearch, Logstash, Kibana)!&lt;/p&gt;

&lt;p&gt;So how does this work?&lt;/p&gt;

&lt;p&gt;The ELK stack is composed of three components&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;E&lt;/strong&gt;lasticsearch&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;L&lt;/strong&gt;ogstash&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;K&lt;/strong&gt;ibana&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Logstash receives, and treats our logs into some format we can run a query on, it forwards them to elasticsearch (that actually stores our logs) and then we use kibana to actually run the searchs and make neat dashboards.&lt;/p&gt;

&lt;p&gt;On this post I’ll get the stack working, on a next post I’ll have it receiving squid logs.&lt;/p&gt;

&lt;h1 id=&quot;so-lets-get-our-hands-dirty&quot;&gt;So lets get our hands dirty.&lt;/h1&gt;

&lt;p&gt;I am using an ubuntu 12.04 server (ya, 14.04 is out I know…) and I’ll be following a &lt;a href=&quot;https://www.digitalocean.com/community/tutorials/how-to-use-logstash-and-kibana-to-centralize-and-visualize-logs-on-ubuntu-14-04&quot;&gt;guide from the awesome people at Digital Ocean&lt;/a&gt;. I absolutely recomend people to read this link, I am basing my lil guide on it, but changing things to make more sense to me here and there.&lt;/p&gt;

&lt;h1 id=&quot;get-your-java&quot;&gt;Get your java&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Use sudo wherever its needed&lt;/strong&gt;, I am doing all this as root, because I’m fidling with a test environment, be a good admin and use sudo on production servers.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;add-apt-repository -y ppa:webupd8team/java
apt-get update
apt-get -y install oracle-java7-installer
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;add-elasticity-to-your-search&quot;&gt;Add elasticity to your search&lt;/h1&gt;

&lt;p&gt;Get it done!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;wget -O - http://packages.elasticsearch.org/GPG-KEY-elasticsearch |  apt-key add -
echo &#39;deb http://packages.elasticsearch.org/elasticsearch/1.1/debian stable main&#39; | tee /etc/apt/sources.list.d/elasticsearch.list
apt-get update
apt-get -y install elasticsearch=1.1.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With that done lets edit elasticsearch’s configuration file&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vi /etc/elasticsearch/elasticsearch.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We will change two lines, one to disable dynamic scripts, and the second to restrict access to our elasticsearch instance to localhost only, so people can’t shutdown our shinny new toy using the HTTP API.&lt;/p&gt;

&lt;p&gt;So, add this line:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;script.disable_dynamic: true
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then uncomment and edit this one (comes with 192.168.0.1 instead of localhost):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;network.host: localhost
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(markdown is awesome, so easy to write blog posts these days…)&lt;/p&gt;

&lt;p&gt;Save the file and start elasticsearch.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;service elasticsearch restart
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Lets have it up on boot (doing as the installer tells us)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;update-rc.d elasticsearch defaults 95 10 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Woot! our searchs are elastic now (yup, I’m in a dad joke/pun mood today)&lt;/p&gt;

&lt;h1 id=&quot;rev-up-your-nginx&quot;&gt;Rev up your Ngin…X&lt;/h1&gt;

&lt;p&gt;Kibana sits on a webserver, my test box has none installed, I love nginx, and the original guide on Digital Ocean uses it too, so lets go with it.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apt-get install nginx
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Damn this was hard. Kibana connects to Elasticsearch on port 9200, we connect to the webserver on port 80, nginx needs to proxy the requests from 80 -&amp;gt; 9200.&lt;/p&gt;

&lt;p&gt;If you’re familiar with nginx this should be trivial, but since the Kibana guys are so nice we will get a sample config from their github repo.&lt;/p&gt;

&lt;p&gt;Get into a temp folder and download it.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;wget https://github.com/elasticsearch/kibana/raw/master/sample/nginx.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Edit it&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vi nginx.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Lets change the following lines to look like this&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;server_name localhost;
root /usr/share/nginx/kibana;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Save it, and lets overwrite the default nginx&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cp nginx.conf /etc/nginx/sites-available/default
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Restart nginx&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;service nginx restart
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;install-kibana-failed-to-think-of-a-crappy-pun&quot;&gt;Install Kibana (failed to think of a crappy pun)&lt;/h1&gt;

&lt;p&gt;Get into a temp folder and download Kibana, then unpack the file.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;wget https://download.elasticsearch.org/kibana/kibana/kibana-3.0.1.tar.gz
tar zxvf kibana-3.0.1.tar.gz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Edit its configuration&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vi kibana-3.0.1/config.js
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the elasticsearch line change it to use port 80 (from 9200).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;elasticsearch: &quot;http://&quot;+window.location.hostname+&quot;:80&quot;,
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Lets rename and move our folder to the place we previously specified in the nginx conf file.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mv kibana-3.0.1 kibana
mv kibana /usr/share/nginx/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now you should be able to see your kibana interface when trying to access http://your_server_ip/.&lt;/p&gt;

&lt;p&gt;If all went well you should be greeted by this nice interface.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/article_images/2014-10-05-ELK-Stack-and-Squid/kibana.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I am not setting up any kind of login screen at this time, we will get to it at the end of this guide (gotta get things working first :D)&lt;/p&gt;

&lt;h1 id=&quot;certificates&quot;&gt;Certificates&lt;/h1&gt;

&lt;p&gt;We will be using Logstash Forwarder to send the logs from our servers to the logstash server, so we will need to create some certificates (yay -_-) those will be used by the forwarder to verify the identity of the logstash server, I like to think of certificates as photo ids :D.&lt;/p&gt;

&lt;p&gt;Lets make the folders to store the certs and create then in the correct place.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mkdir -p /etc/pki/tls/certs
mkdir /etc/pki/tls/private
cd /etc/pki/tls
openssl req -x509 -batch -nodes -days 3650 -newkey rsa:2048 -keyout private/logstash-forwarder.key -out certs/logstash-forwarder.crt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Later we will copy this cert to all the servers that send logs to logstash.&lt;/p&gt;

&lt;h1 id=&quot;a-stash-for-our-logs&quot;&gt;A stash for our logs&lt;/h1&gt;

&lt;p&gt;Installing logstash is mad easy, so get it done&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;echo &#39;deb http://packages.elasticsearch.org/logstash/1.4/debian stable main&#39; | tee /etc/apt/sources.list.d/logstash.list
apt-get update
apt-get install logstash=1.4.2-1-2c0f5a1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ok, its installed, now to configure it.&lt;/p&gt;

&lt;p&gt;So, logstash conf files are separated in inputs, filters and outputs.&lt;/p&gt;

&lt;p&gt;Inputs will describe how the server will receive the logs.
Filters will get your cryptic log lines and change into something you can search later.
Output will describe where the filtered info will be sent to.&lt;/p&gt;

&lt;p&gt;Let’s go ahead and setup a lumberjack input, lumberjack is the protocol logstash forwarder uses.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vi /etc/logstash/conf.d/01-lumberjack-input.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then insert this configuration:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;input {
  lumberjack {
    port =&amp;gt; 5000
    type =&amp;gt; &quot;logs&quot;
    ssl_certificate =&amp;gt; &quot;/etc/pki/tls/certs/logstash-forwarder.crt&quot;
    ssl_key =&amp;gt; &quot;/etc/pki/tls/private/logstash-forwarder.key&quot;
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;What is this doing? the file says it all, we are setting up a lumberjack input in port 5000(tcp), and it will use the ssl cert we created before.&lt;/p&gt;

&lt;p&gt;Now lets create a new file where we will add a filter for syslog messages (syslog, the megalogfile :P).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vi /etc/logstash/conf.d/10-syslog.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Insert this filter conf:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;filter {
  if [type] == &quot;syslog&quot; {
    grok {
      match =&amp;gt; { &quot;message&quot; =&amp;gt; &quot;%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} %{DATA:syslog_program}(?:\[%{POSINT:syslog_pid}\])?: %{GREEDYDATA:syslog_message}&quot; }
      add_field =&amp;gt; [ &quot;received_at&quot;, &quot;%{@timestamp}&quot; ]
      add_field =&amp;gt; [ &quot;received_from&quot;, &quot;%{host}&quot; ]
    }
    syslog_pri { }
    date {
      match =&amp;gt; [ &quot;syslog_timestamp&quot;, &quot;MMM  d HH:mm:ss&quot;, &quot;MMM dd HH:mm:ss&quot; ]
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So, this says that it will filter logs that have been labeled as “syslog” type by logstash forwarder, and it will use grok to parse it into something we can search on later.&lt;/p&gt;

&lt;p&gt;Now the output, once again we create a new file.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vi /etc/logstash/conf.d/30-lumberjack-output.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then we put this in:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;output {
  elasticsearch { host =&amp;gt; localhost }
  stdout { codec =&amp;gt; rubydebug }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This tells logstash to store the logs in elasticsearch.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;If you want to setup more filters, be sure to have the conf files named between input and output ( between our 01 and 30 files ).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Restart it and…&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;service logstash restart
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;TADA! Its working.&lt;/p&gt;

&lt;p&gt;Now what?&lt;/p&gt;

&lt;p&gt;Now we gotta ship these logs from the servers to logstash, meaning we gotta setup logstash forwarder.&lt;/p&gt;

&lt;h1 id=&quot;ship-those-logs&quot;&gt;Ship those logs!!!&lt;/h1&gt;

&lt;p&gt;First of all, remember those ssl certs we created? Yup, gotta copy them to the servers you will have logs shipped to logstash.&lt;/p&gt;

&lt;p&gt;So I like to think I’m a smart guy (not really :P) and made a simple script (you could do this on puppet if you are actually smart ;). Remember that I’m working in a test lab, and I’m mostly interested in getting it running to check out how it works.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;When you work in production be smarter and use something like puppet to deploy this, and dont use root for everything like I’m doing, &lt;strong&gt;sudo exists for a reason&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/bash
mkdir -p /etc/pki/tls/certs
scp root@my_logstash_server:/etc/pki/tls/certs/logstash-forwarder.crt /etc/pki/tls/certs
echo &#39;deb http://packages.elasticsearch.org/logstashforwarder/debian stable main&#39; | tee /etc/apt/sources.list.d/logstashforwarder.list
apt-get update
apt-get install logstash-forwarder
cd /etc/init.d/
wget https://raw.github.com/elasticsearch/logstash-forwarder/master/logstash-forwarder.init -O logstash-forwarder
chmod +x logstash-forwarder
update-rc.d logstash-forwarder defaults
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There it is! Installed, now lets configure it.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vi /etc/logstash-forwarder
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now insert the following lines, replace logstash_server with the ip/name of your logstash server.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &quot;network&quot;: {
    &quot;servers&quot;: [ &quot;logstash_server_private_IP:5000&quot; ],
    &quot;timeout&quot;: 15,
    &quot;ssl ca&quot;: &quot;/etc/pki/tls/certs/logstash-forwarder.crt&quot;
  },
  &quot;files&quot;: [
    {
      &quot;paths&quot;: [
        &quot;/var/log/syslog&quot;,
        &quot;/var/log/auth.log&quot;
       ],
      &quot;fields&quot;: { &quot;type&quot;: &quot;syslog&quot; }
    }
   ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So, this tells the forwarder to connect on our logstash server on port 5000 (remember our server input config?), using the ssl cert.
The paths tell which logfiles to send, in this case syslog and auth.log, and the type label (remember our filter conf?).&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;If you had something like a squid server we could send the squid log files, and label then with type: somethingelse.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Restart it now:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;service logstash-forwarder restart
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now it is sending logs to the server.&lt;/p&gt;

&lt;h1 id=&quot;lets-use-our-new-toy&quot;&gt;Lets use our new toy!&lt;/h1&gt;

&lt;p&gt;Now that you repeated that on all the servers you want to monitor, jump to kibana (http://logstash_server_ip).&lt;/p&gt;

&lt;p&gt;There’s a link to a premade logstash dashboard.&lt;/p&gt;

&lt;p&gt;Now you should have some data popping in, use the query field to search for stuff like a username (to see if someone is hammering a username to bruteforce their way into the server), also fiddle around with the graph functions, have fun ;)&lt;/p&gt;

&lt;p&gt;![]/assets/article_images/2014-10-05-ELK-Stack-and-Squid/kibana-working.png)&lt;/p&gt;

&lt;h1 id=&quot;extra-safe&quot;&gt;Extra safe?&lt;/h1&gt;

&lt;p&gt;Since kibana doesnt have any kind of authentication, we can put a login/pass at the webserver.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apt-get install apache2-utils
htpasswd -c /etc/nginx/conf.d/kibana.myhost.org.htpasswd username
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then enter and verify the password, worry not, this file is already referenced in the nginx install we did before.
Restart nginx to apply&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;service nginx restart
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Thats all for now, I’ll be fidling with squid logs soon and I’ll write another guide for it.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Kudos to &lt;a href=&quot;https://www.digitalocean.com/community/users/manicas&quot;&gt;Mitchell Anicas&lt;/a&gt; for the awesome guide I based this post on. By no means I claim to have done it all myself, I just rewrote it in a way thats easier for me.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Sat, 04 Oct 2014 21:00:00 -0300</pubDate>
        <link>//log_processing/2014/10/04/ELK-Stack-and-Squid.html</link>
        <guid isPermaLink="true">//log_processing/2014/10/04/ELK-Stack-and-Squid.html</guid>
        
        <category>linux</category>
        
        <category>logstash</category>
        
        <category>elasticsearch</category>
        
        <category>kibana</category>
        
        <category>devops</category>
        
        
        <category>log_processing</category>
        
      </item>
    
  </channel>
</rss>
